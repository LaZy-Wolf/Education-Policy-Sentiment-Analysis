{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPUVKRUBxq9CXq/Q4SHbEG9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaZy-Wolf/akki/blob/main/Untitled29.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlgBp7PHRdhr",
        "outputId": "57f4e7f6-2b88-4eef-865c-c3781ff15d96"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.7.9)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    TORCH_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TORCH_AVAILABLE = False\n",
        "    print(\"PyTorch not available. Using CPU for inference.\")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "try:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "except:\n",
        "    logger.warning(\"Some NLTK downloads failed\")\n",
        "\n",
        "class ImprovedSentimentAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.vader_analyzer = SentimentIntensityAnalyzer()\n",
        "        self.bert_model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)\n",
        "        device = 0 if (TORCH_AVAILABLE and torch.cuda.is_available()) else -1\n",
        "        self.bert_classifier = pipeline(\n",
        "            \"sentiment-analysis\",\n",
        "            model=self.bert_model_name,\n",
        "            tokenizer=self.tokenizer,\n",
        "            device=device\n",
        "        )\n",
        "        custom_stop_words = {'nep', 'policy', 'education', 'tweet', 'twitter'}\n",
        "        self.stop_words.update(custom_stop_words)\n",
        "\n",
        "    def advanced_clean_tweet(self, text):\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        text = str(text)\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'@\\w+', '', text)\n",
        "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "        text = re.sub(r'[😀-🙏]', ' positive_emoji ', text)\n",
        "        text = re.sub(r'[😞-😢]', ' negative_emoji ', text)\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.lower().strip()\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens\n",
        "                 if token not in self.stop_words and len(token) > 2]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def get_vader_sentiment(self, text):\n",
        "        scores = self.vader_analyzer.polarity_scores(text)\n",
        "        compound = scores['compound']\n",
        "        if compound >= 0.1:\n",
        "            return 'positive', abs(compound)\n",
        "        elif compound <= -0.1:\n",
        "            return 'negative', abs(compound)\n",
        "        else:\n",
        "            return 'neutral', 1 - abs(compound)\n",
        "\n",
        "    def get_bert_sentiment_batch(self, texts, batch_size=16):\n",
        "        results = []\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing BERT sentiment\"):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            batch = [text[:512] for text in batch]\n",
        "            try:\n",
        "                batch_results = self.bert_classifier(batch)\n",
        "                for result in batch_results:\n",
        "                    label = result['label']\n",
        "                    if label in ['LABEL_2', 'POSITIVE']:\n",
        "                        sentiment = 'positive'\n",
        "                    elif label in ['LABEL_0', 'NEGATIVE']:\n",
        "                        sentiment = 'negative'\n",
        "                    else:\n",
        "                        sentiment = 'neutral'\n",
        "                    results.append((sentiment, result['score']))\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing batch: {e}\")\n",
        "                results.extend([('neutral', 0.5)] * len(batch))\n",
        "        return results\n",
        "\n",
        "    def ensemble_sentiment(self, vader_sentiment, vader_score, bert_sentiment, bert_score):\n",
        "        bert_weight = 0.7\n",
        "        vader_weight = 0.3\n",
        "        sentiment_scores = {\n",
        "            'positive': 1,\n",
        "            'neutral': 0,\n",
        "            'negative': -1\n",
        "        }\n",
        "        vader_numeric = sentiment_scores[vader_sentiment] * vader_score\n",
        "        bert_numeric = sentiment_scores[bert_sentiment] * bert_score\n",
        "        ensemble_score = (bert_weight * bert_numeric + vader_weight * vader_numeric)\n",
        "        if ensemble_score > 0.1:\n",
        "            return 'positive', abs(ensemble_score)\n",
        "        elif ensemble_score < -0.1:\n",
        "            return 'negative', abs(ensemble_score)\n",
        "        else:\n",
        "            return 'neutral', 1 - abs(ensemble_score)\n",
        "\n",
        "def enhanced_visualizations(df):\n",
        "    plt.style.use('default')\n",
        "    try:\n",
        "        sns.set_palette(\"husl\")\n",
        "    except:\n",
        "        pass\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    sns.countplot(data=df, x='ensemble_sentiment', ax=axes[0,0],\n",
        "                  order=['positive', 'neutral', 'negative'])\n",
        "    axes[0,0].set_title('Sentiment Distribution (Ensemble Model)', fontsize=14)\n",
        "    axes[0,0].set_xlabel('Sentiment')\n",
        "    axes[0,0].set_ylabel('Count')\n",
        "    sns.histplot(data=df, x='ensemble_score', hue='ensemble_sentiment',\n",
        "                 bins=30, ax=axes[0,1])\n",
        "    axes[0,1].set_title('Sentiment Score Distribution', fontsize=14)\n",
        "    if 'Date_of_tweet' in df.columns:\n",
        "        try:\n",
        "            df['Date_of_tweet'] = pd.to_datetime(df['Date_of_tweet'])\n",
        "            daily_sentiment = df.groupby([df['Date_of_tweet'].dt.date, 'ensemble_sentiment']).size().unstack(fill_value=0)\n",
        "            daily_sentiment.plot(kind='line', ax=axes[1,0], marker='o')\n",
        "            axes[1,0].set_title('Daily Sentiment Trends', fontsize=14)\n",
        "            axes[1,0].tick_params(axis='x', rotation=45)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not create time series plot: {e}\")\n",
        "            axes[1,0].text(0.5, 0.5, 'Time series data not available',\n",
        "                          horizontalalignment='center', transform=axes[1,0].transAxes)\n",
        "    if 'vader_sentiment' in df.columns and 'bert_sentiment' in df.columns:\n",
        "        comparison_data = pd.DataFrame({\n",
        "            'VADER': df['vader_sentiment'].value_counts(),\n",
        "            'BERT': df['bert_sentiment'].value_counts(),\n",
        "            'Ensemble': df['ensemble_sentiment'].value_counts()\n",
        "        }).fillna(0)\n",
        "        comparison_data.plot(kind='bar', ax=axes[1,1])\n",
        "        axes[1,1].set_title('Model Comparison', fontsize=14)\n",
        "        axes[1,1].tick_params(axis='x', rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('comprehensive_sentiment_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    for sentiment in ['positive', 'negative', 'neutral']:\n",
        "        sentiment_text = ' '.join(df[df['ensemble_sentiment'] == sentiment]['cleaned_text'])\n",
        "        if sentiment_text:\n",
        "            wordcloud = WordCloud(\n",
        "                width=1200,\n",
        "                height=600,\n",
        "                background_color='white',\n",
        "                max_words=100,\n",
        "                collocations=False,\n",
        "                colormap='viridis'\n",
        "            ).generate(sentiment_text)\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.imshow(wordcloud, interpolation='bilinear')\n",
        "            plt.axis('off')\n",
        "            plt.title(f'Word Cloud - {sentiment.title()} Sentiment', fontsize=16)\n",
        "            plt.savefig(f'{sentiment}_wordcloud_enhanced.png', dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "def main():\n",
        "    start_time = datetime.now()\n",
        "    logger.info(\"Starting enhanced sentiment analysis...\")\n",
        "    try:\n",
        "        analyzer = ImprovedSentimentAnalyzer()\n",
        "        df = pd.read_csv('NEP_2020_english_tweet.csv')\n",
        "        logger.info(f\"Loaded {len(df)} tweets\")\n",
        "        df = df.dropna(subset=['Tweet'])\n",
        "        df = df[df['Tweet'].str.len() > 10]\n",
        "        logger.info(f\"After cleaning: {len(df)} tweets\")\n",
        "        logger.info(\"Cleaning tweets...\")\n",
        "        df['cleaned_text'] = df['Tweet'].apply(analyzer.advanced_clean_tweet)\n",
        "        df = df[df['cleaned_text'].str.len() > 0]\n",
        "        logger.info(\"Running VADER sentiment analysis...\")\n",
        "        vader_results = df['cleaned_text'].apply(analyzer.get_vader_sentiment)\n",
        "        df['vader_sentiment'] = vader_results.apply(lambda x: x[0])\n",
        "        df['vader_score'] = vader_results.apply(lambda x: x[1])\n",
        "        logger.info(\"Running BERT sentiment analysis...\")\n",
        "        bert_results = analyzer.get_bert_sentiment_batch(df['cleaned_text'].tolist())\n",
        "        df['bert_sentiment'] = [result[0] for result in bert_results]\n",
        "        df['bert_score'] = [result[1] for result in bert_results]\n",
        "        logger.info(\"Creating ensemble predictions...\")\n",
        "        ensemble_results = df.apply(lambda row: analyzer.ensemble_sentiment(\n",
        "            row['vader_sentiment'], row['vader_score'],\n",
        "            row['bert_sentiment'], row['bert_score']\n",
        "        ), axis=1)\n",
        "        df['ensemble_sentiment'] = ensemble_results.apply(lambda x: x[0])\n",
        "        df['ensemble_score'] = ensemble_results.apply(lambda x: x[1])\n",
        "        logger.info(\"Creating visualizations...\")\n",
        "        enhanced_visualizations(df)\n",
        "        logger.info(\"Calculating performance metrics...\")\n",
        "        vader_bert_agreement = (df['vader_sentiment'] == df['bert_sentiment']).mean()\n",
        "        logger.info(f\"VADER-BERT agreement: {vader_bert_agreement:.3f}\")\n",
        "        sentiment_dist = df['ensemble_sentiment'].value_counts(normalize=True)\n",
        "        logger.info(f\"Sentiment distribution: {sentiment_dist.to_dict()}\")\n",
        "        df.to_csv('enhanced_sentiment_results.csv', index=False)\n",
        "        logger.info(\"Results saved to enhanced_sentiment_results.csv\")\n",
        "        summary = {\n",
        "            'total_tweets': len(df),\n",
        "            'processing_time': str(datetime.now() - start_time),\n",
        "            'model_agreement': vader_bert_agreement,\n",
        "            'sentiment_distribution': sentiment_dist.to_dict()\n",
        "        }\n",
        "        logger.info(\"Analysis complete!\")\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main execution: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    summary = main()\n",
        "    print(f\"Analysis Summary: {summary}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHLVVB3L3BaP",
        "outputId": "9d897bdd-ba66-412b-add5-934fb9d9114e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n",
            "Processing BERT sentiment:   1%|          | 10/1140 [00:02<02:49,  6.66it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Processing BERT sentiment: 100%|██████████| 1140/1140 [02:48<00:00,  6.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis Summary: {'total_tweets': 18238, 'processing_time': '0:03:07.215278', 'model_agreement': np.float64(0.28352889571224915), 'sentiment_distribution': {'positive': 0.5214935848228972, 'neutral': 0.4016887816646562, 'negative': 0.07681763351244654}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tgg1JCjxRu20"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}